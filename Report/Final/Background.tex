%!TEX root = Main.tex
% Background: words
\chapter{Speech Recognition Techniques and Tools} % (fold)
\label{cha:background}

% TODO: I think this background is too focussed on theory.  I hardly mention *any* actual background work... Fix?
% TODO: Did I explain why I decided to use an FPGA???

\section{Speech Recognition Systems} % (fold)
\label{sec:speech_recognition_systems}
	In general, ``Speech Recognition'' refers to the process of translating spoken words or phrases into a form that can be understood by an electronic system, which usually means using mathematical models and methods to process and then decode the sound signal.  Translating a speech waveform into this form typically requires three main steps \cite{melnikoff2003speech}.  The raw waveform must be converted into an ``observation vector'', which is a representative set of data that is compatible with the chosen speech model.  This data is then sent through a decoder, which attempts to recognise which words or sub-word units were spoken.  These are finally processed by language modeller, which imposes rules on what combinations of words of syntax are allowed.  This project focusses on implementing pre-processing, and the first stage of the decoder, as these are interesting tasks from an electronic engineering point of view.

	There are a variety of different methods and models that have been used to perform speech recognition.  An overview of the most popular will be described here, and then the chosen technique (HMMs) is described in Section~\ref{sec:about_hmms}.

	\subsection{Tor's Algorithm} % (fold)
	\label{sub:tors_algorithm}
		``Tor's Algorithm'' is a very simple speech recognition system \cite{tor2003}, capable of accurate speaker dependent speech recognition for a small dictionary of about ten words.  It is based on a fingerprinting model where each word in the dictionary must be trained to form an acoustic ``fingerprint''.  This fingerprint is based on the time variations of the speech signal after being filtered appropriately.  Then recognition is reduced to finding the Euclidean distance squared between the input vector and each of the stored fingerprints.  The `best' match is the word with the smallest distance from the input.
		
		It is likely that this system is easily implementable in this project's time frame, but it was judged too simplistic and therefore not interesting enough to warrant implementing.  However, importantly, it outlines two major components of any speech recognition system -- pre-processing and decoding (recognition).  More complex systems essentially just use more complex speech models and pre-processing methods.
	% subsection tors_algorithm (end)

	\subsection{Dynamic Time Warping} % (fold)
		\label{sub:dynamic_time_warping}
		%TODO: find paper on DTW for ASR, reference here.
		% Background should contain _Lit Reviews_! ie show I've read around.
		Speech, by nature, is not constrained to be at a certain speed -- the duration of words will vary between utterances, and a speech recognition system should be able to handle this.  Dynamic Time Warping (DTW) \nomenclature{DTW}{Dynamic Time Warping} is essentially the process of expanding and contracting the time axis, so that waveforms may be compared, independent of talking speed.  Combined with a dynamic programming technique for finding the optimal `warp' amount, it became a widely used approach to solving the problem of speech duration modelling \cite{furui1989speech}.  One useful property of DTW is that it may offer good performance even with little training, as it only needs one word as a template \cite{melnikoff2003speech}.  Conversely, the performance of DTW based systems cannot be increased much with more training, unlike Hidden Markov Models.
		%TODO: write about about DTW is a special form of HMM (and ref it!)
		% Why mention this??
		Although DTW is better than Tor's algorithm, it is also judged to be relatively old and rarely used technology, and therefore not as interesting as HMMs for a project focus.
	% subsection dynamic_time_warping (end)
% section speech_recognition_systems (end)


\section{Hidden Markov Models} % (fold)
\label{sec:about_hmms}
	By far the most prevalent and successful approach to modern speech recognition uses Hidden Markov Models for the statistical modelling and decoding of speech \cite{cox1988hidden}.  The flexibility inherent in HMMs is key to their success, as a system can be made more and more accurate by simply improving the HMM models or training the models further.  The classic tutorial paper by Rabiner (\cite{rabiner1989tutorial}) is one of the best references for HMMs in speech recognition, and provides a very good overview of modern systems.  However, a brief summary of the fundamentals of HMMs is given here. The following sections are based heavily on \cite{rabiner1989tutorial} and \cite{htkbook}.

	An $N$-state Markov Chain can be described as a finite state machine of $N$ nodes with an $N{\times}N$ matrix of probabilities which define the transitions between each state.  According to the notation in \cite{rabiner1989tutorial}, the elements of this matrix are defined as $a_{ij} = P($state at time $t = j |$state at time $t-1 = i)$.  To make this a Hidden Markov Model, each state is assigned an emission probability for every possible observation, which defines how likely that state will emit that observation.  In this case, the actual position in the state machine is not observable -- only the state emissions are (thus ``Hidden'' Markov Model).  The probability that a state $j$ will emit observation $O$ is defined as $b_j(O)$, and may be either a discrete value or a continuous distribution depending on the nature of the observations.  Thus, an HMM is defined entirely by the matrices $a$ and $b$, and a set of initial probabilities for each state, $\pi$, collectively denoted as $\lambda(\pi,a,b)$.

	For speech recognition, the performance is substantially improved by using continuous HMMs, as it removes the need to quantise the speech data which is, by nature, continuous \cite{matsui1992comparison}.  A common  distribution used for continuous probabilities is the multivariate Gaussian Mixture, which is essentially a weighted summation of several different Normal distributions \cite{bilmes2006hmms}.  However, for use in HMMs, the computational complexity is greatly reduced if the covariance matrix is diagonal (i.e., the components of each Gaussian are uncorrelated).  This requirement can lead to extra pre-processing of observation data in order to remove correlation between the components.

	\subsection{Levels of Complexity} % (fold)
	\label{sub:levels_of_complexity}
		%TODO find papers on 'word based hmm recognisers', etc
		The simplest HMM based systems use a single HMM for every word in the recognition dictionary.  Given a set of observations, each HMM can be scored based on the probability that it would output the observations.  The HMM with the highest score is taken as the recognised word.  The most apparent limitation of this system is that a very large amount of training would be required if a dictionary of substantial size was to be used.  At the very least, one sample of each word would need to be recorded to train the full system, which would be a very time consuming process.  However, for simple applications (voice dialling, for example) this is manageable.

		The next step up in complexity from single word HMMs is models that composed of sub-word utterances (phonemes).  This allows a smaller set of HMMs to be used for much larger dictionary recognition, as words are recognised based on sequences of sub-word HMMs.  Thus instead of searching through a single HMM to recognise a word, the recognition process becomes a search through a trellis of multiple HMMs in order to find the best path through them.  The most simple HMM system of this form is based on mono-phones, of which there are about 50 in the English language.

		Even more complexity (and, potentially, recognition accuracy) can be introduced by using bi- or tri-phone HMMs, which model transitions between two or three mono-phones.  Using this form of HMM will increase the acoustic model size greatly however, as there are many possible combinations of mono-phones in the English language.  However, it allows context dependent scoring of phonemes, including HMMs that model word endings and starts, or silences.  In the Sphinx 3 recognition engine, the internal states of these HMMs are referred to as ``Senones'', and the term has been adopted and used extensively in this project \cite{sphinx}.
	% subsection levels_of_complexity (end)

% section about_hmms (end)


\section{Speech Pre-Processing} % (fold)
\label{sec:speech_pre_processing}
	Speech signals are complex waveforms and cannot be processed without some form of feature extraction which reduces the complexity whilst retaining the important features.  In modern speech recognition systems the two most common methods of analysing and representing speech are: \cite{gaikwad2010review}
	\begin{itemize}
		\item Linear Predictive Coding (LPC)
		\item Mel-Frequency Cepstral Coefficients (MFCCs)
	\end{itemize}
	Both these methods attempt to model the movement and dynamics of the human vocal tract and auditory perception.  LPC is more suited to speaker recognition (the process of identifying voices, rather than words), whilst MFCCs are more useful for speech recognition \cite{sd2012interview}.

	The Mel-Frequency Cepstrum is based on a filterbank analysis with a cepstral transformation, which is required due to the high correlation between filterbank amplitudes.  The human ear perceives sound on a non-linear frequency scale, and one way of improving recognition performance is by using a similar scale for analysis of speech.  A filterbank analysis can be used to perform this discrimination between different frequencies, and the frequency bins are usually spaced using the Mel frequency scale.  However, the filterbank amplitudes are highly correlated, which greatly increases the computational complexity of the HMM based recogniser as the covariance matrix will not be diagonal.  In order to correct this, a discrete linear cosine transform is taken on the log filterbank amplitudes, finally resulting in a set of Mel Frequency Cepstral Coefficients.  The HTK (Section~\ref{sec:the_htk}) defaults to using twelve MFCC filterbank bins. \cite{htkbook} \cite{melnikoff2003speech}

	In order to attain MFCCs, a sampling rate must be chosen such that enough data is gathered while allowing sufficient processing time.  In addition, to perform Fourier transforms on the speech, the incoming signal must be windowed appropriately.  The HTK has a set of default values for these parameters, which are assumed to be appropriate.

	An improvement to both LPC and MFCCs is to compute time derivatives in the feature extraction process, which gives a better idea of how the signal changes over time.  In addition, the log energy of each sample may also be computed to also boost recognition ability.
	%TODO: reference this more??
% section speech_pre_processing (end)


\section{The HTK and VoxForge} % (fold)
\label{sec:the_htk}
	The Hidden Markov Model Toolkit (HTK) is a set of tools and libraries for developing and testing HMMs, primarily for speech processing and recognition tools \cite{htkbook}.  Given a model structure and a set of transcribed speech recordings (a speech corpus), a set of HMMs may be trained using the HTK.  This includes performing all pre-processing in a number of formats, and testing recognition capabilities of a model \cite{woodland1994htk}.

	Voxforge is an open source speech corpus which is aimed at facilitating speech recognition development.  It provides pre-compiled acoustic models -- essentially large sets of HMMs -- in the format created by HTK, licensed under the GPL (GNU General Public License) \cite{voxforge}.  The alternative would be to use another speech corpus (such as TIMIT \cite{timit}), and then use the HTK to design and train the acoustic model.  This is potentially a very time consuming process, so Voxforge is useful because it essentially cuts this step out.  In addition, the Voxforge model may be easily adapted to a specific person's voice using only a few minutes of transcribed speech.  However, the model is very complex, with \~8000 tri-phone context-dependent HMMs with multivariate Gaussian output probabilities.  Thus, implementing a recogniser system based on this model requires a lot more work than if a simpler model was used, such as one based on discrete (output probability) HMMs.  However, modern speech recognisers are likely to use a model that is as complex, if not more so.
% section the_htk (end)


\section{Embedded Hardware and Speech Silicon} % (fold)
\label{sec:embedded_hardware}
	A wide range of speech recognition software (commercial and open source) exists for desktop computers or laptops.  However, speech recognition for embedded systems is less widespread.  In a real speaker independent, context-dependent system there could be thousands of these states, each requiring a large number of calculations, depending on how complex the models are.  The processing power required for this is often not available on embedded processors.  Recently there has been increased research into the use of DSPs and FPGAs for speech recognition \cite{nedevschi2005hardware}; of particular interest is Stephen Melnikoff's PhD Thesis \cite{melnikoff2003speech}, and the Speech Silicon architecture \cite{schuster2006speech}.  The former investigates a variety of HMM based recognisers on an FPGA, using a PC to perform pre and post processing.  The latter details a data driven FPGA based architecture capable of performing recognition on medium-sized vocabularies.

	These two systems are good guides for what is possible and important to implement in a speech recognition system.  In addition, they present efficient ways of performing certain tasks, such as Gaussian distance calculations.  Several parts of the system implemented in this project were influenced by the way similar parts were implemented by Melnikoff and Speech Silicon.  However, both Melnikoff and Speech Silicon perform an in-depth analysis of an entire speech recognition system based on programmable logic.  As such, both of them require relatively large FPGAs -- far larger than the Micro Arcana FPGA board.  Thus they were more useful from a theoretical point of view, rather than architecturally.
% section embedded_hardware (end)


% \section{Personal Contribution} % (fold)
% \label{sec:personal_contribution}
% 	% What did I do?
% 	% This is primarily a POC system
% 	The initial goal of the project, to build a complete speech recognition system, was very ambitious and had to be narrowed down as more was learnt about the complexities of modern speech recognition systems.  Instead of attempting to build a full system, it was decided that development would focus on the decoding stage of recognition.  Speech pre-processing is a very established process, and implementing it is usually a case of linking together the appropriate libraries.

% 	The result of this project is a system spread across two development boards from the Micro Arcana: L'Imperatrice and La Papessa.  
% 	This involved...
% 		- 

% 		- Benefit from developing my knowledge of SV, speech recognition, intelligent algorithms, etc
% 		- Providing an application for the Micro Arcana family, and showing that it can do stuff
% 		- Providing a solid basis for future work in the area
% % section personal_contribution (end)



% chapter background (end)